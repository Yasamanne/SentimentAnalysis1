{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e667787b-2e14-408b-aef5-b0efc441e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Yasaman Emami\"\n",
    "__email__ = ['emami.yasamann@gmail.com','yasaman.emami@sjsu.edu']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda40ee7-a987-4a68-b601-46069ccc33fa",
   "metadata": {},
   "source": [
    "# Create spark session and spark dataframe from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "564e816f-d19a-4d1c-981e-a8bf02b063b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/23 21:05:39 WARN Utils: Your hostname, YasamanEms-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.250 instead (on interface en0)\n",
      "21/12/23 21:05:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/12/23 21:05:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------------------+-------+--------------------+-------+--------------+--------------+-----------+\n",
      "|    reviewerID|      asin|       reviewerName|helpful|          reviewText|overall|       summary|unixReviewTime| reviewTime|\n",
      "+--------------+----------+-------------------+-------+--------------------+-------+--------------+--------------+-----------+\n",
      "|A240ORQ2LF9LUI|0077613252|         Michelle W|   null|The materials arr...|    4.0|Material Great|    1394496000|03 11, 2014|\n",
      "|A1YCCU0YRLS0FE|0077613252|Rosalind White Ames|   null|I am really enjoy...|    4.0|        Health|    1393113600|02 23, 2014|\n",
      "+--------------+----------+-------------------+-------+--------------------+-------+--------------+--------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "  \n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "      StructField(\"reviewerID\",StringType(),True),\n",
    "      StructField(\"asin\",StringType(),True),\n",
    "      StructField(\"reviewerName\",StringType(),True),\n",
    "      StructField(\"helpful\",StringType(),True),\n",
    "      StructField(\"reviewText\",StringType(),True),\n",
    "      StructField(\"overall\",StringType(),True),\n",
    "      StructField(\"summary\",StringType(),True),\n",
    "      StructField(\"unixReviewTime\",StringType(),True),\n",
    "      StructField(\"reviewTime\",StringType(),True)\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).json('data/Software.json')\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ec4c2-f40b-42c2-9206-d06f22c06f01",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa3fe02-571d-46a5-b90c-551be8298c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "#manual feature selection\n",
    "df = df.selectExpr(\"cast(reviewText as string) reviewText\",\n",
    "                   \"cast(overall as int) overall\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6572b7-3cc9-4fb2-be67-2222da2ba23f",
   "metadata": {},
   "source": [
    "### Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f2dd0a-889a-46e4-8086-9d4b7cc156c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop any rows contain null\n",
    "df = df.na.drop(\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0657b-0431-45d0-b3a1-93ab0c5e98bc",
   "metadata": {},
   "source": [
    "### Checking the number of records for each overall rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b59675-3fbe-4256-96f1-bd1e3d3ed667",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|count(overall)|overall|\n",
      "+--------------+-------+\n",
      "|        102542|      1|\n",
      "|         39394|      3|\n",
      "|        212399|      5|\n",
      "|         73590|      4|\n",
      "|         31445|      2|\n",
      "+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# creating a temporary view of\n",
    "# Dataframe and storing it into df2\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# using the SQL query to count all\n",
    "# distinct records and display the\n",
    "# count on the screen\n",
    "spark.sql(\"select count((overall)),overall from df group by overall\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd3acb-1106-4c65-8d32-15a25b7d6186",
   "metadata": {},
   "source": [
    "### Filter rating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beaebf12-a23a-4112-b07a-b8f15ba973f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter records, with rating score of 3 because they are neutral\n",
    "# filter records, with rating value if out of [0,5] since it would be invalid\n",
    "df = df.where(\"overall<6 and overall!=3\")\n",
    "df = df.where(\"overall>0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe8318-53a6-443a-86e2-1927d063e196",
   "metadata": {},
   "source": [
    "### Cheking scores after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43c32960-49be-422f-818a-eff7b8b6805d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|count(overall)|overall|\n",
      "+--------------+-------+\n",
      "|        102542|      1|\n",
      "|        212399|      5|\n",
      "|         73590|      4|\n",
      "|         31445|      2|\n",
      "+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# creating a temporary view of\n",
    "# Dataframe and storing it into df2\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# using the SQL query to count all\n",
    "# distinct records and display the\n",
    "# count on the screen\n",
    "spark.sql(\"select count((overall)),overall from df group by overall\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ef4ce-4afb-44b2-8e13-afde1ee675cc",
   "metadata": {},
   "source": [
    "### Bucketizing overall scores to two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fd4f879-c2b0-403b-aef4-39fbf90fdeeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+\n",
      "|          reviewText|overall|category|\n",
      "+--------------------+-------+--------+\n",
      "|The materials arr...|      4|     1.0|\n",
      "|I am really enjoy...|      4|     1.0|\n",
      "|IF YOU ARE TAKING...|      1|     0.0|\n",
      "|I have used Learn...|      5|     1.0|\n",
      "|Strong backgroung...|      4|     1.0|\n",
      "|i got this book o...|      5|     1.0|\n",
      "|I was very happy ...|      5|     1.0|\n",
      "|Recieved in a tim...|      5|     1.0|\n",
      "|Maybe it's just m...|      2|     0.0|\n",
      "|This was the text...|      5|     1.0|\n",
      "|Not worth the pri...|      2|     0.0|\n",
      "|I love how this b...|      4|     1.0|\n",
      "|Great on the deli...|      5|     1.0|\n",
      "|The book was deli...|      5|     1.0|\n",
      "|Required to buy t...|      2|     0.0|\n",
      "|Didn't help me mu...|      1|     0.0|\n",
      "|Disappointing tex...|      1|     0.0|\n",
      "|This book provide...|      4|     1.0|\n",
      "|I've been using D...|      4|     1.0|\n",
      "|The demo is done ...|      4|     1.0|\n",
      "+--------------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "#Bucketizing sentiments into two categories (0 for neg and 1 for pos)\n",
    "bucketizer = Bucketizer(splits=[ 1, 4, 5 ],inputCol=\"overall\", outputCol=\"category\")\n",
    "df = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a5fe5-3fe7-4150-b06d-cc3152128ccd",
   "metadata": {},
   "source": [
    "### Replacing regex and convert sentiments to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "314bb1ce-a67d-4d88-bf93-5bd282a3af5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sq\n",
    "from pyspark.sql.functions import lower, col\n",
    "\n",
    "#convert txt to lower case\n",
    "df = df.select(\"*\", lower(col('reviewText')).alias(\"lower_text\"))\n",
    "#remove new lines\n",
    "df = df.withColumn(\"no_line_text\", sq.regexp_replace(\"lower_text\", r\"\\n\", \" \"))\n",
    "#only keep words contains a-z\n",
    "df = df.withColumn(\"removed_punc_text\", sq.regexp_replace(\"no_line_text\", r\"[^a-z]\", \" \"))\n",
    "#replace multi space with one \n",
    "df = df.withColumn(\"text_ready\", sq.regexp_replace(\"removed_punc_text\", r\" +\", ' '))\n",
    "#remove single character from string\n",
    "df = df.withColumn(\"final_txt\", sq.regexp_replace(\"text_ready\",r\"\\b[a-zA-Z]\\b\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e02bce4-7397-44a8-9f78-116b2f427f39",
   "metadata": {},
   "source": [
    "### Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9319f35b-2213-4333-94cd-975358e59a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " # of records including duplicates:  419976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n # of records including duplicates:  \" + str(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc4c1e39-8f9c-4aa7-a89f-986eefb3d92d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4deb371e-c08a-46cf-80e9-ec0c2a40d269",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===============================================>      (175 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " # of records after drop duplicates:  385876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n # of records after drop duplicates:  \" + str(df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1672e21c-6744-4bdc-89a1-8df539a84294",
   "metadata": {},
   "source": [
    "### Shuffling records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ac30c5d-afa4-4d2e-a7d2-af71305e034b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand \n",
    "#shuffling data\n",
    "df = df.orderBy(rand())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61afa8c4-72c2-4872-ae54-cb55a1d78beb",
   "metadata": {},
   "source": [
    "### Formating labels as fasttext requires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18a21d0d-5185-4328-91e9-ea0446745eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "label_names = {0.0:\"__label__negative\",1.0:\"__label__positive\"}\n",
    "udf_cat = udf(lambda x: label_names[x], StringType())\n",
    "\n",
    "df = df.withColumn(\"label\", udf_cat(\"category\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d05a75-13d9-49eb-b54b-2e95d2739cbf",
   "metadata": {},
   "source": [
    "### Cheking count of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efc7d841-80cb-4c75-aee0-2d3cca08deab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|count(label)|            label|\n",
      "+------------+-----------------+\n",
      "|      256690|__label__positive|\n",
      "|      129186|__label__negative|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating sparksession and giving app name\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "# creating a temporary view of\n",
    "# Dataframe and storing it into df2\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# using the SQL query to count all\n",
    "# distinct records and display the\n",
    "# count on the screen\n",
    "spark.sql(\"select count((label)),label from df group by label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72655d5d-366f-485a-9985-cd6e38adf650",
   "metadata": {},
   "source": [
    "# Split Data to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2307d2e8-d387-4c0c-9817-8c7e5d2c3b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|          reviewText|overall|category|          lower_text|        no_line_text|   removed_punc_text|          text_ready|           final_txt|            label|\n",
      "+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|\"I love Webroot, ...|      5|     1.0|\"i love webroot, ...|\"i love webroot, ...| i love webroot  ...| i love webroot i...|  love webroot it...|__label__positive|\n",
      "|(This item should...|      4|     1.0|(this item should...|(this item should...| this item should...| this item should...| this item should...|__label__positive|\n",
      "|....no complaints...|      4|     1.0|....no complaints...|....no complaints...|    no complaints...| no complaints he...| no complaints he...|__label__positive|\n",
      "|1) Not only is th...|      1|     0.0|1) not only is th...|1) not only is th...|   not only is th...| not only is the ...| not only is the ...|__label__negative|\n",
      "|100% of what I ne...|      5|     1.0|100% of what i ne...|100% of what i ne...|     of what i ne...| of what i needed...| of what  needed ...|__label__positive|\n",
      "|4yo enjoys it. Pr...|      4|     1.0|4yo enjoys it. pr...|4yo enjoys it. pr...| yo enjoys it  pr...| yo enjoys it pre...| yo enjoys it pre...|__label__positive|\n",
      "|5-22-2008:\n",
      "Seems ...|      5|     1.0|5-22-2008:\n",
      "seems ...|5-22-2008: seems ...|           seems ...| seems to do what...| seems to do what...|__label__positive|\n",
      "|98% accuracy out ...|      1|     0.0|98% accuracy out ...|98% accuracy out ...|    accuracy out ...| accuracy out of ...| accuracy out of ...|__label__negative|\n",
      "|<a data-hook=\"pro...|      2|     0.0|<a data-hook=\"pro...|<a data-hook=\"pro...| a data hook  pro...| a data hook prod...|  data hook produ...|__label__negative|\n",
      "|<a data-hook=\"pro...|      4|     1.0|<a data-hook=\"pro...|<a data-hook=\"pro...| a data hook  pro...| a data hook prod...|  data hook produ...|__label__positive|\n",
      "|<div id=\"video-bl...|      1|     0.0|<div id=\"video-bl...|<div id=\"video-bl...| div id  video bl...| div id video blo...| div id video blo...|__label__negative|\n",
      "|A fantastic produ...|      5|     1.0|a fantastic produ...|a fantastic produ...|a fantastic produ...|a fantastic produ...| fantastic produc...|__label__positive|\n",
      "|A great introduct...|      5|     1.0|a great introduct...|a great introduct...|a great introduct...|a great introduct...| great introducti...|__label__positive|\n",
      "|A great product a...|      5|     1.0|a great product a...|a great product a...|a great product a...|a great product a...| great product at...|__label__positive|\n",
      "|A little expensiv...|      5|     1.0|a little expensiv...|a little expensiv...|a little expensiv...|a little expensiv...| little expensive...|__label__positive|\n",
      "|A superior securi...|      5|     1.0|a superior securi...|a superior securi...|a superior securi...|a superior securi...| superior securit...|__label__positive|\n",
      "|A very easy to in...|      4|     1.0|a very easy to in...|a very easy to in...|a very easy to in...|a very easy to in...| very easy to ins...|__label__positive|\n",
      "|A victim of a mal...|      5|     1.0|a victim of a mal...|a victim of a mal...|a victim of a mal...|a victim of a mal...| victim of  malic...|__label__positive|\n",
      "|About as good as ...|      5|     1.0|about as good as ...|about as good as ...|about as good as ...|about as good as ...|about as good as ...|__label__positive|\n",
      "|Absolute GARBAGE!...|      1|     0.0|absolute garbage!...|absolute garbage!...|absolute garbage ...|absolute garbage ...|absolute garbage ...|__label__negative|\n",
      "+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(train_data, test_data) = df.randomSplit([0.7, 0.3], seed = 100)\n",
    "train_data.show(truncate = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d93d4a8-0235-4433-baaf-928eb79e0416",
   "metadata": {},
   "source": [
    "# Concatintating label and asociated sentiment to create text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98743f29-3677-46cb-b069-18e539e1a9bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat,col,lit\n",
    "\n",
    "train_df = train_data.select(concat(train_data.label,lit(\" \"),train_data.final_txt).alias(\"all\"))\n",
    "\n",
    "test_df = test_data.select(concat(test_data.label,lit(\" \"),test_data.final_txt).alias(\"all\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b1bcc09-19d8-4347-9ece-2b6091a1bee3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|          reviewText|overall|category|          lower_text|        no_line_text|   removed_punc_text|          text_ready|           final_txt|            label|\n",
      "+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|\"I love Webroot, ...|      5|     1.0|\"i love webroot, ...|\"i love webroot, ...| i love webroot  ...| i love webroot i...|  love webroot it...|__label__positive|\n",
      "|(This item should...|      4|     1.0|(this item should...|(this item should...| this item should...| this item should...| this item should...|__label__positive|\n",
      "|....no complaints...|      4|     1.0|....no complaints...|....no complaints...|    no complaints...| no complaints he...| no complaints he...|__label__positive|\n",
      "|1) Not only is th...|      1|     0.0|1) not only is th...|1) not only is th...|   not only is th...| not only is the ...| not only is the ...|__label__negative|\n",
      "|100% of what I ne...|      5|     1.0|100% of what i ne...|100% of what i ne...|     of what i ne...| of what i needed...| of what  needed ...|__label__positive|\n",
      "|4yo enjoys it. Pr...|      4|     1.0|4yo enjoys it. pr...|4yo enjoys it. pr...| yo enjoys it  pr...| yo enjoys it pre...| yo enjoys it pre...|__label__positive|\n",
      "|5-22-2008:\n",
      "Seems ...|      5|     1.0|5-22-2008:\n",
      "seems ...|5-22-2008: seems ...|           seems ...| seems to do what...| seems to do what...|__label__positive|\n",
      "|98% accuracy out ...|      1|     0.0|98% accuracy out ...|98% accuracy out ...|    accuracy out ...| accuracy out of ...| accuracy out of ...|__label__negative|\n",
      "|<a data-hook=\"pro...|      2|     0.0|<a data-hook=\"pro...|<a data-hook=\"pro...| a data hook  pro...| a data hook prod...|  data hook produ...|__label__negative|\n",
      "|<a data-hook=\"pro...|      4|     1.0|<a data-hook=\"pro...|<a data-hook=\"pro...| a data hook  pro...| a data hook prod...|  data hook produ...|__label__positive|\n",
      "|<div id=\"video-bl...|      1|     0.0|<div id=\"video-bl...|<div id=\"video-bl...| div id  video bl...| div id video blo...| div id video blo...|__label__negative|\n",
      "|A fantastic produ...|      5|     1.0|a fantastic produ...|a fantastic produ...|a fantastic produ...|a fantastic produ...| fantastic produc...|__label__positive|\n",
      "|A great introduct...|      5|     1.0|a great introduct...|a great introduct...|a great introduct...|a great introduct...| great introducti...|__label__positive|\n",
      "|A great product a...|      5|     1.0|a great product a...|a great product a...|a great product a...|a great product a...| great product at...|__label__positive|\n",
      "|A little expensiv...|      5|     1.0|a little expensiv...|a little expensiv...|a little expensiv...|a little expensiv...| little expensive...|__label__positive|\n",
      "|A superior securi...|      5|     1.0|a superior securi...|a superior securi...|a superior securi...|a superior securi...| superior securit...|__label__positive|\n",
      "|A very easy to in...|      4|     1.0|a very easy to in...|a very easy to in...|a very easy to in...|a very easy to in...| very easy to ins...|__label__positive|\n",
      "|A victim of a mal...|      5|     1.0|a victim of a mal...|a victim of a mal...|a victim of a mal...|a victim of a mal...| victim of  malic...|__label__positive|\n",
      "|About as good as ...|      5|     1.0|about as good as ...|about as good as ...|about as good as ...|about as good as ...|about as good as ...|__label__positive|\n",
      "|Absolute GARBAGE!...|      1|     0.0|absolute garbage!...|absolute garbage!...|absolute garbage ...|absolute garbage ...|absolute garbage ...|__label__negative|\n",
      "+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data.show(truncate = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90c555cc-9f28-4482-bc44-e25597cb4304",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|                                                         all|\n",
      "+------------------------------------------------------------+\n",
      "|__label__positive   love webroot it is very easy to use  ...|\n",
      "|__label__positive  this item should be called   protector...|\n",
      "|__label__positive  no complaints here to complain about d...|\n",
      "|__label__negative  not only is the program core so old th...|\n",
      "|__label__positive  of what  needed in one very easy to us...|\n",
      "|  __label__positive  yo enjoys it pretty much as advertised |\n",
      "|__label__positive  seems to do what is supposed to do and...|\n",
      "|__label__negative  accuracy out of the box is an embellis...|\n",
      "|__label__negative   data hook product link linked class  ...|\n",
      "|__label__positive   data hook product link linked class  ...|\n",
      "|__label__negative  div id video block  hg ay  uj class  s...|\n",
      "|__label__positive  fantastic product well known and prove...|\n",
      "|__label__positive  great introduction to  great opening f...|\n",
      "|__label__positive  great product at  great price  have us...|\n",
      "|           __label__positive  little expensive but worth it |\n",
      "|__label__positive  superior security program for all my d...|\n",
      "|__label__positive  very easy to install and manage anti v...|\n",
      "|__label__positive  victim of  malicious hacker  was quick...|\n",
      "|__label__positive about as good as you can get and the pr...|\n",
      "|__label__negative absolute garbage am an experienced pc u...|\n",
      "+------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.show(truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b87d85-1208-41be-819e-03501b2ed6aa",
   "metadata": {},
   "source": [
    "## Write datasets to text file as fasttext requires txt input file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f3d82ac-097e-429e-bc19-fe0a608ec51a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 448:=================================================>       (7 + 1) / 8]\r"
     ]
    }
   ],
   "source": [
    "textfile = open(\"train_data.txt\", \"w\")\n",
    "i =0\n",
    "for row in list(train_df.toLocalIterator()):\n",
    "    i += 1\n",
    "    textfile.write(row['all'] + \"\\n\")\n",
    "\n",
    "textfile.close()\n",
    "\n",
    "textfile = open(\"test_data.txt\", \"w\")\n",
    "i =0\n",
    "for row in list(test_df.toLocalIterator()):\n",
    "    i += 1\n",
    "    textfile.write(row['all'] + \"\\n\")\n",
    "\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85c3daee-9e71-4006-9969-421630fbbfa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " number of training records: 270252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 852:=================================================>   (186 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " number of testing records: 115624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n number of training records: \" + str(train_df.count()))\n",
    "print(\"\\n number of testing records: \" + str(test_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80812d89-68c7-4e91-959f-b58c6ebaf5d8",
   "metadata": {},
   "source": [
    "# Training fasttext classification model, manully set the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cd1b55c-ebdf-47ca-9ce5-a317c049d856",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 20M words\n",
      "Number of words:  92029\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 4281731 lr:  0.000000 avg.loss:  0.239810 ETA:   0h 0m 0s  7.8% words/sec/thread: 4681253 lr:  0.092204 avg.loss:  0.294875 ETA:   0h 0m14s 22.3% words/sec/thread: 4602498 lr:  0.077706 avg.loss:  0.269207 ETA:   0h 0m12s 25.2% words/sec/thread: 4554056 lr:  0.074824 avg.loss:  0.266023 ETA:   0h 0m12s 27.7% words/sec/thread: 4553673 lr:  0.072324 avg.loss:  0.262984 ETA:   0h 0m11s 29.5% words/sec/thread: 4545873 lr:  0.070505 avg.loss:  0.262554 ETA:   0h 0m11s 43.8% words/sec/thread: 4530173 lr:  0.056223 avg.loss:  0.255645 ETA:   0h 0m 9s 57.4% words/sec/thread: 4524343 lr:  0.042561 avg.loss:  0.250131 ETA:   0h 0m 6s 59.9% words/sec/thread: 4516677 lr:  0.040097 avg.loss:  0.249405 ETA:   0h 0m 6s 79.1% words/sec/thread: 4405701 lr:  0.020851 avg.loss:  0.244399 ETA:   0h 0m 3s 90.7% words/sec/thread: 4348068 lr:  0.009264 avg.loss:  0.241498 ETA:   0h 0m 1s 94.5% words/sec/thread: 4329027 lr:  0.005458 avg.loss:  0.240804 ETA:   0h 0m 0s 98.4% words/sec/thread: 4316578 lr:  0.001582 avg.loss:  0.240022 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "model_manual_params = fasttext.train_supervised(input=\"train_data.txt\", lr=0.1, epoch=25, wordNgrams=1, bucket=200000, dim=10, loss='hs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c13f38-235d-4e5a-b754-c3c526dedf74",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training fasttext classification model with auto hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ead94ea6-84b1-4e94-bd18-c7aba9f200f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:   10 Best score:  0.934763 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 20M words\n",
      "Number of words:  92029\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  735878 lr:  0.000000 avg.loss:  0.132907 ETA:   0h 0m 0s  4.2% words/sec/thread:  985178 lr:  0.056579 avg.loss:  0.400784 ETA:   0h 0m25s 20.9% words/sec/thread:  835530 lr:  0.046688 avg.loss:  0.260779 ETA:   0h 0m24s 22.8% words/sec/thread:  779921 lr:  0.045570 avg.loss:  0.255020 ETA:   0h 0m26s 23.4% words/sec/thread:  760837 lr:  0.045205 avg.loss:  0.253466 ETA:   0h 0m26s 25.5% words/sec/thread:  682903 lr:  0.043973 avg.loss:  0.246096 ETA:   0h 0m28s 26.8% words/sec/thread:  656416 lr:  0.043216 avg.loss:  0.242038 ETA:   0h 0m29s 30.9% words/sec/thread:  656499 lr:  0.040818 avg.loss:  0.230053 ETA:   0h 0m27s 36.7% words/sec/thread:  679773 lr:  0.037381 avg.loss:  0.216059 ETA:   0h 0m24s 37.3% words/sec/thread:  681827 lr:  0.037006 avg.loss:  0.214493 ETA:   0h 0m24s 38.1% words/sec/thread:  686639 lr:  0.036542 avg.loss:  0.213211 ETA:   0h 0m23s 41.3% words/sec/thread:  696133 lr:  0.034656 avg.loss:  0.206731 ETA:   0h 0m22s 50.3% words/sec/thread:  745858 lr:  0.029341 avg.loss:  0.190728 ETA:   0h 0m17s 53.6% words/sec/thread:  755699 lr:  0.027415 avg.loss:  0.185051 ETA:   0h 0m16s 55.5% words/sec/thread:  758884 lr:  0.026247 avg.loss:  0.181490 ETA:   0h 0m15s 56.2% words/sec/thread:  760112 lr:  0.025849 avg.loss:  0.180649 ETA:   0h 0m15s 62.2% words/sec/thread:  743815 lr:  0.022344 avg.loss:  0.171714 ETA:   0h 0m13s 63.1% words/sec/thread:  738493 lr:  0.021772 avg.loss:  0.170780 ETA:   0h 0m13s 67.6% words/sec/thread:  716042 lr:  0.019156 avg.loss:  0.164743 ETA:   0h 0m11s 70.4% words/sec/thread:  711749 lr:  0.017454 avg.loss:  0.160678 ETA:   0h 0m10s 73.1% words/sec/thread:  710749 lr:  0.015901 avg.loss:  0.157701 ETA:   0h 0m 9s 77.1% words/sec/thread:  709990 lr:  0.013523 avg.loss:  0.153220 ETA:   0h 0m 8s8s 84.6% words/sec/thread:  710693 lr:  0.009114 avg.loss:  0.146184 ETA:   0h 0m 5s 86.1% words/sec/thread:  709830 lr:  0.008207 avg.loss:  0.144636 ETA:   0h 0m 5s 89.7% words/sec/thread:  717082 lr:  0.006072 avg.loss:  0.141131 ETA:   0h 0m 3s\n"
     ]
    }
   ],
   "source": [
    "model_auto_hyper = fasttext.train_supervised(input=\"train_data.txt\", autotuneValidationFile='test_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92bffb-686a-41e4-a2cb-bb8e7a4d1ea5",
   "metadata": {},
   "source": [
    "## Computing Percision & Recall for binary classes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72f79589-b431-4c4f-8979-1842fa8e5126",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 858:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------+-----------------------------------+---------------------------------+\n",
      "|          reviewText|overall|category|          lower_text|        no_line_text|   removed_punc_text|          text_ready|           final_txt|            label|manual_hyper_param_model_prediction|auto_hyper_param_model_prediction|\n",
      "+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------+-----------------------------------+---------------------------------+\n",
      "|....talk about be...|      1|     0.0|....talk about be...|....talk about be...|    talk about be...| talk about being...| talk about being...|__label__negative|                  __label__negative|                __label__negative|\n",
      "|<a data-hook=\"pro...|      1|     0.0|<a data-hook=\"pro...|<a data-hook=\"pro...| a data hook  pro...| a data hook prod...|  data hook produ...|__label__negative|                  __label__positive|                __label__positive|\n",
      "|<a data-hook=\"pro...|      4|     1.0|<a data-hook=\"pro...|<a data-hook=\"pro...| a data hook  pro...| a data hook prod...|  data hook produ...|__label__positive|                  __label__positive|                __label__positive|\n",
      "|A CHEAP IMATATION...|      1|     0.0|a cheap imatation...|a cheap imatation...|a cheap imatation...|a cheap imatation...| cheap imatation ...|__label__negative|                  __label__negative|                __label__negative|\n",
      "|A client of mine ...|      1|     0.0|a client of mine ...|a client of mine ...|a client of mine ...|a client of mine ...| client of mine p...|__label__negative|                  __label__negative|                __label__negative|\n",
      "|A lifesaver.  Wit...|      5|     1.0|a lifesaver.  wit...|a lifesaver.  wit...|a lifesaver   wit...|a lifesaver with ...| lifesaver with t...|__label__positive|                  __label__positive|                __label__positive|\n",
      "|A little difficul...|      4|     1.0|a little difficul...|a little difficul...|a little difficul...|a little difficul...| little difficult...|__label__positive|                  __label__positive|                __label__positive|\n",
      "|ALL PRODUCT WORKS...|      5|     1.0|all product works...|all product works...|all product works...|all product works...|all product works...|__label__positive|                  __label__positive|                __label__positive|\n",
      "|Absolutely imposs...|      1|     0.0|absolutely imposs...|absolutely imposs...|absolutely imposs...|absolutely imposs...|absolutely imposs...|__label__negative|                  __label__negative|                __label__negative|\n",
      "|Accidentally lock...|      1|     0.0|accidentally lock...|accidentally lock...|accidentally lock...|accidentally lock...|accidentally lock...|__label__negative|                  __label__negative|                __label__negative|\n",
      "|According to the ...|      1|     0.0|according to the ...|according to the ...|according to the ...|according to the ...|according to the ...|__label__negative|                  __label__negative|                __label__negative|\n",
      "|Accounting progra...|      2|     0.0|accounting progra...|accounting progra...|accounting progra...|accounting progra...|accounting progra...|__label__negative|                  __label__negative|                __label__negative|\n",
      "|Acronis is buggy,...|      1|     0.0|acronis is buggy,...|acronis is buggy,...|acronis is buggy ...|acronis is buggy ...|acronis is buggy ...|__label__negative|                  __label__negative|                __label__negative|\n",
      "|Advantages:\n",
      "I hav...|      4|     1.0|advantages:\n",
      "i hav...|advantages: i hav...|advantages  i hav...|advantages i have...|advantages  have ...|__label__positive|                  __label__positive|                __label__positive|\n",
      "|After I bought th...|      4|     1.0|after i bought th...|after i bought th...|after i bought th...|after i bought th...|after  bought thi...|__label__positive|                  __label__positive|                __label__positive|\n",
      "|After long and sa...|      4|     1.0|after long and sa...|after long and sa...|after long and sa...|after long and sa...|after long and sa...|__label__positive|                  __label__negative|                __label__negative|\n",
      "|After purchasing ...|      1|     0.0|after purchasing ...|after purchasing ...|after purchasing ...|after purchasing ...|after purchasing ...|__label__negative|                  __label__negative|                __label__negative|\n",
      "|After studying Ru...|      4|     1.0|after studying ru...|after studying ru...|after studying ru...|after studying ru...|after studying ru...|__label__positive|                  __label__positive|                __label__positive|\n",
      "|After trying to i...|      1|     0.0|after trying to i...|after trying to i...|after trying to i...|after trying to i...|after trying to i...|__label__negative|                  __label__negative|                __label__negative|\n",
      "|All I can say is ...|      5|     1.0|all i can say is ...|all i can say is ...|all i can say is ...|all i can say is ...|all  can say is i...|__label__positive|                  __label__positive|                __label__positive|\n",
      "+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------+-----------------------------------+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, monotonically_increasing_id\n",
    "\n",
    "test = test_data.select('text_ready').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "#predict labels based on manual hyper parameters model and auto hyper parameters tunning\n",
    "pred = model_manual_params.predict(test)\n",
    "pred_auto = model_auto_hyper.predict(test)\n",
    "\n",
    "# insert predicted labels column from manual param set model to test_data dataframe as \"prediction\" column\n",
    "test_data = test_data.repartition(1).withColumn(\n",
    "                                        \"manual_hyper_param_model_prediction\", \n",
    "                                        udf(lambda id: ' '.join(pred[0][id]))(monotonically_increasing_id())\n",
    ")\n",
    "# insert predicted labels column from auto hyper param model to test_data dataframe as \"prediction_auto_m\" column\n",
    "test_data = test_data.repartition(1).withColumn(\n",
    "                                        \"auto_hyper_param_model_prediction\", \n",
    "                                        udf(lambda id: ' '.join(pred_auto[0][id]))(monotonically_increasing_id())\n",
    ")\n",
    "\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30a8d96-7c4a-402d-984c-b2ba3f745ad2",
   "metadata": {},
   "source": [
    "### manual parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f7fec3a-4557-4fdb-90af-b2b1dac59516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115624, 0.9107797689061095, 0.9107797689061095)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual param model accuracy level\n",
    "model_manual_params.test(\"test_data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1597d359-3dad-45f0-a2f4-fc770a42d623",
   "metadata": {},
   "source": [
    "### auto hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "480c0754-e2bd-4f21-81c8-f86dc16c3aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115624, 0.9345724071127102, 0.9345724071127102)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# auto hyper params model accuracy level\n",
    "model_auto_hyper.test(\"test_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6024101-a8c2-46bf-b4c7-7cd5bd9acb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives manual_hyper_param_model_prediction 72231\n",
      "True Negatives manual_hyper_param_model_prediction 33077\n",
      "False Positives manual_hyper_param_model_prediction 5667\n",
      "False Negatives manual_hyper_param_model_prediction 4649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  115624\n",
      "\n",
      "manual_hyper_param_model_prediction recall 0.9395291363163372\n",
      "\n",
      "manual_hyper_param_model_prediction precision 0.9272510205653547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives auto_hyper_param_model_prediction 73078\n",
      "True Negatives auto_hyper_param_model_prediction 34708\n",
      "False Positives auto_hyper_param_model_prediction 4036\n",
      "False Negatives auto_hyper_param_model_prediction 3802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 896:=============================================>       (173 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  115624\n",
      "\n",
      "auto_hyper_param_model_prediction recall 0.9505463059313215\n",
      "\n",
      "auto_hyper_param_model_prediction precision 0.9476619031563659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# calculating recall and precision for both models\n",
    "predicted_cols = ['manual_hyper_param_model_prediction', 'auto_hyper_param_model_prediction']\n",
    "\n",
    "for pred in predicted_cols:\n",
    "    tp = test_data[(test_data.label == \"__label__positive\") & (test_data[pred] == \"__label__positive\")].count()\n",
    "    tn = test_data[(test_data.label == \"__label__negative\") & (test_data[pred] == \"__label__negative\")].count()\n",
    "    fp = test_data[(test_data.label == \"__label__negative\") & (test_data[pred] == \"__label__positive\")].count()\n",
    "    fn = test_data[(test_data.label == \"__label__positive\") & (test_data[pred] == \"__label__negative\")].count()\n",
    "    print (\"True Positives\", pred, str(tp))\n",
    "    print (\"True Negatives\", pred,str(tn))\n",
    "    print (\"False Positives\", pred,str(fp))\n",
    "    print (\"False Negatives\", pred,str(fn))\n",
    "    print (\"Total: \", str(test_data.count()))\n",
    "\n",
    "    r = float(tp)/(tp + fn)\n",
    "    print (\"\\n\" + pred + \" recall\", str(r))\n",
    "\n",
    "    p = float(tp) / (tp + fp)\n",
    "    print (\"\\n\" + pred + \" precision\", str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058dab2-ce92-4f34-be9a-cd1f9db0e4c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Getting auto hyperparameters values for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c44f9650-9b7a-45cf-bdcb-3bf012e90b21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autotuneDuration -> 300\n",
      "autotuneMetric -> f1\n",
      "autotuneModelSize -> \n",
      "autotunePredictions -> 1\n",
      "autotuneValidationFile -> test_data.txt\n",
      "bucket -> 5429115\n",
      "cutoff -> 0\n",
      "dim -> 61\n",
      "dsub -> 2\n",
      "epoch -> 9\n",
      "input -> train_data.txt\n",
      "label -> __label__\n",
      "loss -> loss_name.softmax\n",
      "lr -> 0.059040920313827926\n",
      "lrUpdateRate -> 100\n",
      "maxn -> 0\n",
      "minCount -> 1\n",
      "minCountLabel -> 0\n",
      "minn -> 0\n",
      "model -> model_name.supervised\n",
      "neg -> 5\n",
      "output -> \n",
      "pretrainedVectors -> \n",
      "qnorm -> False\n",
      "qout -> False\n",
      "retrain -> False\n",
      "saveOutput -> False\n",
      "seed -> 0\n",
      "setManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x7fd40c017130>>\n",
      "t -> 0.0001\n",
      "thread -> 7\n",
      "verbose -> 2\n",
      "wordNgrams -> 3\n",
      "ws -> 5\n"
     ]
    }
   ],
   "source": [
    "args_obj = model_auto_hyper.f.getArgs()\n",
    "for hparam in dir(args_obj):\n",
    "    if not hparam.startswith('__'):\n",
    "        print(f\"{hparam} -> {getattr(args_obj, hparam)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a634fb-b2fe-4d18-a2ec-a2a194fb0710",
   "metadata": {},
   "source": [
    "## Saving model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa5486d6-6b88-4240-a54d-6e6568c56347",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_auto_hyper.save_model(\"model_amazon_sentiments.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d7e9a6-7d8c-4b4f-96a7-e0c0d3f19abc",
   "metadata": {},
   "source": [
    "## Predicting signle sentiments to better see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69b95a22-3a59-475e-8973-2023c23b9932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative', '__label__positive'),\n",
       " array([1.00001001e+00, 1.00000034e-05]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k parameter is giving the confidence of each label\n",
    "model_auto_hyper.predict(\" worst customer delivery\",k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "063c2ee1-9119-4504-8c69-f6a551ef2d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([1.00001001]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_auto_hyper.predict(\" terrible quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2e78e49-e642-42d4-8edb-c086c7cc48f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__positive',), array([0.99811327]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_auto_hyper.predict(\" very high-quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e64b9548-e065-4025-b071-4f97c6d10b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([1.00001001]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_auto_hyper.predict(\" poor toys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd8b3259-41b7-44ca-b55e-6426488dcb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__positive',), array([1.00001001]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_auto_hyper.predict(\" nice color of many different items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "117c8514-11f7-4190-bce3-cce7cfd6f3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__positive',), array([0.9976359]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_auto_hyper.predict(\"dont hesitate to buy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b2d0498-4a57-4918-a395-24d282a2e7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([1.00000989]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_auto_hyper.predict(\"never buy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f181345b-4bf0-4c08-898b-9f3582835940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__positive',), array([0.68580037]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_auto_hyper.predict(\"came to my door tear up and broken but it seems it was good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "359bad04-8262-47e3-b175-b61e8acb8bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([0.97905266]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_auto_hyper.predict(\"came to my door tear up and broken \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e49f2eb-2441-42a0-acc7-d2ab7f159771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__positive',), array([0.9533323]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_auto_hyper.predict(\"it was awufully good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d74fa336-72bf-4725-92ba-0fff9cbad22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([1.00001001]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_auto_hyper.predict(\"it sucks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f62481a-c9eb-494f-966c-df75872d6366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
